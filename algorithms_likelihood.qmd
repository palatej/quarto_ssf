# Likelihood

## Stationary models

When the initial state of the model is completely specified, the Kalman filter provides the exact likelihood in a trivial way, using the prediction error decomposition:

$$ p(y_0, \ldots, y_{n-1}) = p(y_0) p(y_1\vert y_0) \ldots p(y_{n-1}\vert y_0 \ldots, y_{n-2})$$

Using the notations defined for the ordinary filter, we have:

### univariate case:

$$\log l(\theta, \sigma^2 \vert y)=-1/2\left(n \log(2 \pi) + n \log(\sigma^2)  +  \frac{1}{\sigma^2}\sum_{i=0}^{i<n} e_i^2/f_i + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right)$$
By concentrating $\sigma^2$ out of the likelihood and by writing $RSS=\sum_{i=1}^n e_i^2/f_i$, we obtain :

$$
\begin{align*}
\hat{\sigma}^2 &= \frac{RSS}{n} \\
\log l_c(\theta \vert y)&=-1/2\left(n \log(2 \pi) + n - n \log(n) + n \log(RSS) + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right)
\end{align*}
$$

### multi-variate case:

if we write $N=\sum_{i=0}^n{T_i}$ where $T_i$ is the number of observations for the period $i$ and $RSS=\sum_{i=1}^n u_i'u_i$, we get:

$$
\begin{align*}
\log l(\theta, \sigma^2 \vert y) &=-1/2\left(N \log(2 \pi) + N \log(\sigma^2)  +  \frac{1}{\sigma^2}\sum_{i=0}^{i<n} u_i'u_i + 2\sum_{i=0}^{i<n} {\log \vert R_i \vert }\right) \\
\hat{\sigma}^2 &= \frac{RSS}{N} \\
\log l_c(\theta \vert y) &=-1/2\left(N \log(2 \pi) + N  -N \log(N) + N\log(RSS) + 2\sum_{i=0}^{i<n} {\log \vert R_i \vert }\right) 
\end{align*}
$$


## Diffuse models

### State space models as linear regression models

The general linear gaussian state space model can be transformed in the regression model

$$z = y - c = X \delta + \mu  $$ where

$$c_t= Z_t\left[ \left(\prod_{i=0}^{i<t}{T_i}\right)\right]b \quad X_t = Z_t\left[ \left(\prod_{i=0}^{i<t}{T_i}\right)\right]B $$

and the covariance matrix $\sigma^2\Omega$ of $\mu$ depends on the structure of the state space model

### Profile likelihood

### Diffuse likelihood

### Marginal likelihood

### Implementation

The univariate version of the transformation of the diffuse elements in regression variables is implemented by the method `jdplus.toolkit.base.core.ssf.diffuseEffects` and the transformation of $a_0$ by `jdplus.toolkit.base.core.ssf.initialEffect`

When the covariance matrix of the process is defined up to the scaling factor $\sigma^2$, we define the diffuse likelihood as

$$l(y\vert \theta, \sigma^2) = \lim_{k \rightarrow \infty} \left(l(y \vert k, \theta,\sigma^2)  (2 \pi \sigma^2 k)^{d/2}\right)$$

This is similar to Francke et al.(2010), (equation 14). Apart from the scaling factor, it is also identical to the approach of Ansley and Kohn (1985) (theorem 5.1).

Otherwise, we use the usual definition:

$$l(y\vert \theta) = \lim_{k \rightarrow \infty} \left(l(y \vert k, \theta)  (2 \pi  k)^{d/2}\right)$$

### Durbin-Koopman approach

The expression of the log-likelihood for stationary model as to be corrected by the term

$$ -1/2 \sum_{i=1}^d {\log \vert F_{\infty, i} \vert }$$

to get the diffuse likelihood by means of the approach of [Durbin and Koopman](dk/md). To simplify the notations, we considered above that the diffuse initialization is performed with the first $d$ observations. See DK for the general case.

### De Jong approach

## Bibliography

ANSLEY F. C. and KOHN R. (1985), “Estimating, filtering and smoothing in state space models with incompletely specified initial conditions”, The annals of statistics, vol. 13, n°4, 1286-1316.

DURBIN J. AND KOOPMAN S.J. (2012): "Time Series Analysis by State Space Methods", second edition. Oxford University Press.

FRANCKE, M. K., KOOPMAN, S. J. and DE VOS, A. F. (2010), “Likelihood functions for state space models with diffuse initial conditions”, Journal of Time Series Analysis, 31, 6.

KOHN R. AND ANSLEY F. C. (1985), “Efficient estimation and prediction in time series regression models”, Biometrika, 72, 694-697.
