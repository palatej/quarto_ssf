# Likelihood

We write $l$ the log of the likelihood

## Stationary models

When the initial state of the model is completely specified, the Kalman filter provides the exact likelihood in a trivial way, using the prediction error decomposition:

$$ p(y_0, \ldots, y_{n-1}) = p(y_0) p(y_1\vert y_0) \ldots p(y_{n-1}\vert y_0 \ldots, y_{n-2})$$

Using the notations defined for the ordinary filter, we have:

### univariate case:

$$
l(y;\theta, \sigma^2)=-1/2\left(n \log(2 \pi) + n \log(\sigma^2)  +  \frac{1}{\sigma^2}\sum_{i=0}^{i<n} e_i^2/f_i + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right)
$$

By concentrating $\sigma^2$ out of the likelihood and by writing $RSS=\sum_{i=1}^n e_i^2/f_i$, we obtain :

$$
\begin{align*}
\hat{\sigma}^2 &= \frac{RSS}{n} \\
l_c(y; \theta)&=-1/2\left(n \log(2 \pi) + n - n \log(n) + n \log(RSS) + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right)
\end{align*}
$$

### multi-variate case:

if we write $N=\sum_{i=0}^n{T_i}$ where $T_i$ is the number of observations for the period $i$ and $RSS=\sum_{i=1}^n u_i'u_i$, we get:

$$
\begin{align*}
l(y; \theta, \sigma^2) &=-1/2\left(N \log(2 \pi) + N \log(\sigma^2)  +  \frac{1}{\sigma^2}\sum_{i=0}^{i<n} u_i'u_i + 2\sum_{i=0}^{i<n} {\log \vert R_i \vert }\right) \\
\hat{\sigma}^2 &= \frac{RSS}{N} \\
l_c(y; \theta) &=-1/2\left(N \log(2 \pi) + N  -N \log(N) + N\log(RSS) + 2\sum_{i=0}^{i<n} {\log \vert R_i \vert }\right) 
\end{align*}
$$

## Diffuse models

### State space models as linear regression models

The general linear gaussian state space model can be transformed in the regression model

$$z = y - c = X \delta + \mu  $$ where

$$c_t= Z_t\left[ \left(\prod_{i=0}^{i<t}{T_i}\right)\right]b \quad X_t = Z_t\left[ \left(\prod_{i=0}^{i<t}{T_i}\right)\right]B $$

and the covariance matrix $\sigma^2\Omega$ of $\mu$ depends on the structure of the state space model.

As discussed in the paragraph ..., the Kalman filter defines the linear transformation: $z \rightarrow e_z/\sqrt{f}$. We write $z_L$ the result of that transformation.

We present below the univariate case. The multi-variate models are handled in a similar way.

### Profile likelihood

When $\delta$ is considered as fixed unknown, the log-likelihood is:

$$
l(y;\theta, \delta, \sigma^2)=-1/2\left(n \log(2 \pi) + n \log(\sigma^2)  +  \frac{1}{\sigma^2}\sum_{i=0}^{i<n} (y_i-c_i-X_i \delta)_L^2 + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right)
$$

$(y-c-X \delta)_L = (y-c)_L - X_L \delta$ and we can concentrate $\delta$ and $\sigma^2$ out of the likelihood.

$$ \begin{align*}
\hat{\delta} &= (X_L'X_L)^{-1}X'_L(y-c)_L \\
\hat{e}_L &= (y-c)_L - X_L \hat{\delta} \\
RSS &= \hat{e}_L' \hat{e}_L \\
\hat{\sigma}^2 &= \frac{RSS}{n} \\
l_c(y; \theta)&=-1/2\left(n \log(2 \pi) + n - n \log(n) + n \log(RSS) + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right)
\end{align*} 
$$

### Diffuse likelihood

When $\delta$ is random ( $\delta \sim N(0, \kappa \sigma^2 I)$ ), the log-likelihood is given by:

$$
l(y; \theta, \sigma^2) = l(y \vert \delta; \theta, \sigma^2) + l(\delta; \theta, \sigma^2) - l(\delta \vert y; \theta , \sigma^2)  
$$

If d is the dimension of $\delta$, we have that:

$$
\begin{align*}
l(\delta; \theta, \sigma^2) &= -1/2 \left(d \log(2 \pi) + d \log(\sigma^2) + d \log(\kappa) + \frac{1}{\kappa \sigma^2}{\delta'\delta}  \right) \\
l(y \vert \delta, \theta, \sigma^2) &= -1/2\left(n \log(2 \pi) + n \log(\sigma^2)  +  \frac{1}{\sigma^2}(y -c - X \delta)_L'(y -c - X \delta)_L + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right) \\ 
E(\delta \vert y; \theta, \sigma^2) &= (\frac{1}{\kappa}I + {X_L}' X_L)^{-1}{X_L}'(y-c)_L = \Delta_\kappa^{-1}{X_L}'y^c_L \\
var(\delta \vert; \theta, \sigma^2) &= \sigma^2 (\frac{1}{\kappa}I + {X_L}' X_L)^{-1} = \sigma^2  \Delta_\kappa^{-1}\\
l(\delta \vert y; \theta , \sigma^2) &=  -1/2 \left(d \log(2 \pi) + d \log(\sigma^2) - \log \vert \Delta_\kappa \vert + \frac{1}{\sigma^2}\delta' \Delta_\kappa \delta + \frac{1}{\sigma^2}{y^c_L}'X_L \Delta_\kappa^{-1}{X_L}'y^c_L -\frac{2}{\sigma^2} {y^c_L}'X_L \delta  \right)
\end{align*}
$$

Following Francke et al, we define the diffuse log-likelihood as:

$$
l_\infty(y; \theta, \sigma^2) = \lim_{\kappa \rightarrow \infty}{l(y;\theta, \sigma^2) + \frac{1}{2}\log(\vert 2 \pi \sigma^2 \kappa I)  \vert} 
$$

so that, writing $m=n-d$

$$
l_\infty(y; \theta, \sigma^2) = -1/2 \left( m \log(2 \pi) + m - m \log m + m \log RSS + \log \vert {X_L}' X_L \vert + \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right)
$$

### Marginal likelihood

The marginal likelihood is the likelihood of $y^*=Ay$, which doesn't depend on $\delta$. The transformation A has dimensions $n\times m$ and is such that $A'X=0$

$$
\begin{align*}
l(y^*; \theta, \sigma^2) &= -1/2\left(m \log 2\pi + m \log \sigma^2 + \log({A_L}'A_L)  + \frac{1}{\sigma^2}{y^c}'A({A_L}'A_L)^{-1}A'y^c\right) \\
l(y^*; \theta, \sigma^2) &= -1/2\left(m \log 2\pi + m \log \sigma^2 + \log \vert {X_L}'X_L \vert +\frac{1}{\sigma^2} RSS  - \log \vert X'X \vert  +  \sum_{i=0}^{i<n} {\log \vert f_i \vert }\right) \\
l(y^*; \theta, \sigma^2) &= -1/2\left(m \log 2\pi + m - m \log m + m \log RSS + \log \vert {X_L}' X_L \vert - \log \vert X'X \vert + \sum_{i=0}^{i<n} {\log \vert f_i \vert } \right)
\end{align*}
$$

### Implementation

The univariate version of the transformation of the diffuse elements in regression variables is implemented by the method `jdplus.toolkit.base.core.ssf.diffuseEffects` and the transformation of $a_0$ by `jdplus.toolkit.base.core.ssf.initialEffect`

The different likelihood functions are implemented in `jdplus.toolkit.base.core.ssf.akf.QRFilter`. It should be noted that the RSS are computed in that class by means of a QR decomposition applied on the variables filtered by the Kalman filter.

## Bibliography

ANSLEY F. C. and KOHN R. (1985), “Estimating, filtering and smoothing in state space models with incompletely specified initial conditions”, The annals of statistics, vol. 13, n°4, 1286-1316.

DURBIN J. AND KOOPMAN S.J. (2012): "Time Series Analysis by State Space Methods", second edition. Oxford University Press.

FRANCKE, M. K., KOOPMAN, S. J. and DE VOS, A. F. (2010), “Likelihood functions for state space models with diffuse initial conditions”, Journal of Time Series Analysis, 31, 6.

KOHN R. AND ANSLEY F. C. (1985), “Efficient estimation and prediction in time series regression models”, Biometrika, 72, 694-697.
